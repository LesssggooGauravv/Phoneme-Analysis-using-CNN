# ğŸ—£ï¸ Phoneme Analysis using Convolutional Neural Networks (CNN)

This project implements a **phoneme category classification system** using **pure Convolutional Neural Networks (CNNs)** and **mel-spectrogramâ€“based audio features**.  
It focuses on **robust audio preprocessing, deterministic augmentation, leakage-safe evaluation**, and **phonetic error analysis**.

The project is designed to demonstrate **practical signal-processing knowledge** and **sound ML evaluation practices**, rather than chasing inflated accuracy numbers.

---

## ğŸ“Œ Problem Statement

Phonemes are the smallest distinguishable units of sound in spoken language.  
Many phonemes exhibit **high acoustic similarity**, overlapping formants, and subtle temporal differences, making automatic classification a challenging task.

This project aims to:
- Classify phonemes into **7 broad phoneme categories**
- Analyze **confusion patterns between acoustically similar phonemes**
- Evaluate the effect of **augmentation and feature engineering**
- Build a **leakage-safe CNN pipeline** with realistic performance metrics

---

## ğŸ™ï¸ Phoneme Dataset (Self-Recorded)

> **Important:**  
> The phoneme dataset used in this project was **created entirely using my own voice recordings**.

### Dataset Characteristics
- **Source:** Self-recorded speech samples
- **Speaker:** Single speaker (author)
- **Format:** WAV files
- **Sampling Rate:** 22,050 Hz
- **Organization:** Phonemes grouped into linguistic categories

Using a self-recorded dataset allows:
- Full control over pronunciation and articulation
- Clean phoneme isolation
- Transparent discussion of dataset limitations
- Ethical and licensing clarity

---

## ğŸ§  Phoneme Categories

The dataset is organized into **7 phoneme groups**:

| Category | Description |
|--------|-------------|
| Plosives | p, t, k, b, d, g |
| Nasals | m, n, Å‹ |
| Affricates | tÊƒ, dÊ’ |
| Fricatives | f, v, Î¸, Ã°, s, z, Êƒ, Ê’, h |
| Approximants | w, l, r, j |
| Diphthongs | Vowel glides |
| Monophthongs | Pure vowel sounds |

Each category contains multiple `.wav` files recorded separately.

---

## ğŸµ Feature Extraction

Each audio file is converted into a **3-channel timeâ€“frequency representation**:

### Features Used
- **Mel-Spectrogram** (128 mel bands)
- **Delta (Î”)** â€“ first-order temporal derivative
- **Delta-Delta (Î”Î”)** â€“ second-order temporal derivative

Final input shape:
(128, Time, 3)

These features allow the CNN to learn:
- Spectral envelopes
- Temporal transitions
- Articulatory dynamics of speech sounds

---

## ğŸ”„ Data Augmentation Strategy

To improve robustness and generalization, **deterministic audio augmentation** is applied **only to the training set**.

### Augmentation Techniques
- Pitch shifting (Â±2 semitones)
- Time stretching (0.9Ã—, 1.1Ã—)
- Original (no augmentation)

âš ï¸ **Data Leakage Prevention**
- Trainâ€“test split is performed **before augmentation**
- Test data is **never augmented**
- Normalization statistics are **fit on training data only**

This ensures **realistic and defensible evaluation**.

---

## ğŸ§ª Dataset Preparation Pipeline

- Augmented samples per phoneme class: **100**
- Variable-length audio padded to a common time dimension
- Feature normalization using `StandardScaler`
- Stratified trainâ€“test split

This pipeline prioritizes **stability, reproducibility, and correctness**.

---

## ğŸ¤– Model Architecture (Pure CNN)

The classifier uses a **pure CNN architecture** (no RNNs, no attention):

Input (128 Ã— T Ã— 3)

â†“

Conv2D (32 filters) + BatchNorm + ReLU + MaxPool

â†“

Conv2D (64 filters) + BatchNorm + ReLU + MaxPool

â†“

Conv2D (96 filters) + BatchNorm + ReLU

â†“

Global Average Pooling

â†“

Dense (64) + Dropout

â†“

Dense (7) + Softmax


### Design Rationale
- CNNs capture local spectro-temporal patterns effectively
- Global Average Pooling reduces overfitting
- Moderate depth matches dataset scale
- Batch normalization stabilizes training

---

## ğŸ“Š Evaluation Metrics

The model is evaluated using:
- Accuracy
- Precision, Recall, F1-Score
- Confusion Matrix
- Training vs Validation curves

### Typical Performance
- **Leakage-safe validation accuracy:** ~65â€“75%
- Higher scores achievable with leakage, but not reported
- Strong diagonal dominance in confusion matrices

---

## ğŸ” Phoneme Confusion Analysis

Observed confusions align with phonetic theory:

- **Affricates â†” Fricatives** (shared noise components)
- **Monophthongs â†” Diphthongs** (formant transitions)
- **Approximants â†” Vowels** (similar spectral envelopes)

This indicates the model is learning **meaningful acoustic representations**, not memorizing data.

---

## ğŸ“ˆ Visualizations Included

- Mel-spectrogram visualizations
- Training vs validation accuracy curves
- Training vs validation loss curves
- Confusion matrices (raw and normalized)
- Sample predictions on original phoneme files

---

## ğŸ§ª Experiments Conducted

- Feature ablation (Mel vs Mel+Î” vs Mel+Î”+Î”Î”)
- Model depth vs generalization
- Augmented vs clean evaluation
- Leakage-safe vs leakage-prone pipelines

---

## ğŸ› ï¸ Technologies Used

- Python
- Librosa
- TensorFlow / Keras
- NumPy
- scikit-learn
- Matplotlib / Seaborn
- Jupyter Notebook

---

## ğŸ“Œ Key Takeaways

- CNNs can effectively model phoneme-level acoustics
- Proper evaluation matters more than high accuracy
- Feature engineering significantly improves stability
- Confusion analysis provides linguistic insight
- Self-recorded datasets are viable with correct methodology

---

## ğŸ“„ License

This project is released under the **MIT License**.

---

## ğŸ“¬ Contact

For questions, feedback, or collaboration ideas, please open an issue on GitHub.

